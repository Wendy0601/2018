{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from file 'E:/05_research/04_deep_learning_model/Save_parameters/modelC/modelC_fully02'...\n",
      "INFO:tensorflow:Restoring parameters from E:/05_research/04_deep_learning_model/Save_parameters/modelC/modelC_fully02\n",
      "Loading validation data...\n",
      "Validating model...\n",
      "Overall validation accuracy is 0.704437 \n",
      "\n",
      "86.24338624338624\n",
      "54.976303317535546\n",
      "71.64502164502164\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys \n",
    "import numpy as np \n",
    "import tensorflow as tf\n",
    "import operator\n",
    "batch_size =50 \n",
    "\n",
    "def load_model(session, save_path):\n",
    "    \"\"\" Loads a saved TF model from a file.Returns:The inputs placehoder. \"\"\"\n",
    "    print(\"Loading model from file '%s'...\" % (save_path))\n",
    "    meta_file = save_path + \".meta\"\n",
    "    if not os.path.exists(meta_file):\n",
    "        print(\"ERROR: Expected .meta file '%s', but could not find it.\" % \\\n",
    "        (meta_file))\n",
    "        sys.exit(1) \n",
    "    saver = tf.train.import_meta_graph(meta_file)\n",
    "    save_path = os.path.join(\"./\", save_path)\n",
    "    saver.restore(session, save_path)  \n",
    "    return extract_validation_handles(session)\n",
    "    \n",
    "def load_validation_data(model, testname):\n",
    "    global batch_size \n",
    "    \"\"\" Loads the validation data,  Returns: A tuple of the loaded validation data and validation labels. \"\"\"\n",
    "    print(\"Loading validation data...\")\n",
    "    folder = 'time_series_data/' + testname \n",
    "    if model ==1:\n",
    "        testX =np.load('E:/identification_data/01_PSSE_datasets/Dataset/saved_data_jupyter/'+folder+'/testX_pred.npy' ) \n",
    "        testY =np.load('E:/identification_data/01_PSSE_datasets/Dataset/saved_data_jupyter/'+folder+'/testY_pred.npy'  )  \n",
    "    elif model==2:\n",
    "        testX =np.load('E:/identification_data/01_PSSE_datasets/Dataset/saved_data_jupyter/'+folder+'/testX_no.npy' ) \n",
    "        testY =np.load('E:/identification_data/01_PSSE_datasets/Dataset/saved_data_jupyter/'+folder+'/testY_no.npy' )  \n",
    "    if testX.shape[1]!= 214:\n",
    "        testX = np.transpose(testX,[0,2,1])\n",
    "        testX = np.expand_dims(testX,3) \n",
    "    batch_size =np.shape(testX)[0]   \n",
    "    return (testX,testY )  \n",
    "\n",
    "def validate_model(session, val_data, x, y,  phase_train,prob): \n",
    "    \"\"\" Validates the model stored in a session.Returns:The overall validation accuracy for the model. \"\"\"\n",
    "    global batch_size\n",
    "    print(\"Validating model...\") \n",
    "    predict_op = tf.argmax(prob, 1)\n",
    "    correct = tf.equal(predict_op,tf.argmax(y, 1))\n",
    "    acc= tf.reduce_mean(tf.cast(correct, tf.float32))  \n",
    "    val_x, val_y  = val_data\n",
    "    test_correct = session.run(correct,feed_dict ={x: val_x,  y:val_y, phase_train: True})\n",
    "    test_acc = session.run(acc,feed_dict ={x: val_x, y:val_y, phase_train: True})\n",
    "    return ( test_acc), test_correct  \n",
    "\n",
    "def extract_validation_handles(session):\n",
    "    \"\"\" Extracts the input that we use for validation.\n",
    "    Args:\n",
    "      session: The session with the loaded graph.\n",
    "    Returns:\n",
    "      The inputs placeholders and probability \"\"\" \n",
    "    valid_nodes = tf.get_collection_ref(\"validation_nodes\") \n",
    "    x = valid_nodes[0] \n",
    "    y = valid_nodes[1] \n",
    "    phase_train =valid_nodes[2]\n",
    "    prob =  valid_nodes[3]\n",
    "    global batch_size    \n",
    "    return (x,y,  phase_train,prob)\n",
    "\n",
    "def each_perform(correct_results,eval_labels ):\n",
    "    label_y=np.argmax(eval_labels,1) \n",
    "    \n",
    "    acc = []\n",
    "    zero = np.where(label_y==0) \n",
    "    correctl=[correct_results[i] for i in zero]\n",
    "    accuracy_zero= np.mean(correctl,dtype=np.float32)\n",
    "    acc.append(accuracy_zero)\n",
    "\n",
    "    one = np.where(label_y==1)\n",
    "    correctg=[correct_results[i] for i in one] \n",
    "    accuracy_one= np.mean(correctg,dtype=np.float32)\n",
    "    acc.append(accuracy_one)\n",
    "\n",
    "    two = np.where(label_y==2)\n",
    "    correctt=[correct_results[i] for i in two] \n",
    "    accuracy_two= np.mean(correctt,dtype=np.float32) \n",
    "    acc.append(accuracy_two) \n",
    "    return acc, correctl,correctg,correctt\n",
    "\n",
    "def reset():\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "model = 1\n",
    "testname = 'total'  \n",
    "names = 'modelC_fully02' \n",
    "with tf.Session() as session:  \n",
    "    save_path = 'E:/05_research/04_deep_learning_model/Save_parameters/modelC/'+ names \n",
    "    x, y,phase_train,prob  = load_model(session, save_path)\n",
    "    val_data = load_validation_data(model, testname) # the predicted mode\n",
    "    accuracy ,correct_results= validate_model(session, val_data, x,y, phase_train,prob)\n",
    "    print (\"Overall validation accuracy is %f \\n\" %(accuracy))\n",
    "    acc,correct_l,correct_g,correct_t= each_perform(correct_results,val_data[1] ) \n",
    "    session.close() \n",
    "    #reset() \n",
    "\n",
    "print (100*np.mean(correct_l[0] ))\n",
    "print (100*np.mean(correct_g[0]))\n",
    "print (100*np.mean(correct_t[0] ))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([   0,    1,    2,    7,    8,   13,   19,   20,   25,   26,   27,\n",
       "          32,   33,   38,   39,   40,   45,   46,   51,   52,   53,   58,\n",
       "          59,   64,   65,   66,   71,   72,   90,   91,   92,   95,   97,\n",
       "          98,  103,  104,  105,  109,  111,  112,  117,  118,  119,  120,\n",
       "         124,  125,  130,  131,  132,  133,  137,  138,  143,  144,  145,\n",
       "         146,  150,  151,  152,  156,  157,  158,  163,  164,  169,  170,\n",
       "         171,  172,  175,  176,  177,  178,  188,  196,  197,  198,  199,\n",
       "         200,  201,  202,  204,  205,  206,  215,  216,  218,  226,  227,\n",
       "         229,  230,  231,  232,  233,  234,  236,  237,  238,  239,  240,\n",
       "         244,  245,  247,  251,  252,  253,  256,  257,  258,  260,  263,\n",
       "         272,  274,  275,  341,  342,  343,  344,  345,  346,  347,  349,\n",
       "         350,  351,  352,  353,  370,  371,  372,  373,  374,  375,  376,\n",
       "         378,  380,  390,  398,  399,  400,  401,  402,  403,  404,  407,\n",
       "         409,  410,  419,  427,  428,  429,  430,  431,  435,  447,  457,\n",
       "         458,  459,  460,  461,  462,  463,  465,  466,  467,  468,  469,\n",
       "         478,  486,  487,  488,  489,  490,  491,  492,  494,  496,  497,\n",
       "         505,  506,  514,  515,  516,  517,  518,  519,  520,  523,  524,\n",
       "         526,  543,  544,  547,  549,  562,  563,  587,  633,  665,  666,\n",
       "         667,  668,  669,  670,  671,  672,  673,  674,  675,  676,  677,\n",
       "         678,  679,  680,  681,  682,  683,  684,  685,  686,  687,  688,\n",
       "         689,  690,  691,  692,  693,  694,  695,  696,  697,  698,  699,\n",
       "         700,  701,  702,  703,  704,  705,  706,  707,  708,  709,  710,\n",
       "         725,  792,  827,  861,  962,  963,  964,  969,  974,  975,  976,\n",
       "         977,  981,  982,  986,  987,  988,  989,  993,  994,  998, 1005,\n",
       "        1010, 1011, 1012, 1013, 1016, 1017, 1018, 1019, 1020, 1021, 1022,\n",
       "        1023, 1024, 1025, 1029, 1030, 1031, 1034, 1035, 1036, 1037, 1041,\n",
       "        1042, 1046, 1047, 1049, 1053, 1054, 1056, 1059, 1123, 1136, 1141,\n",
       "        1143, 1146, 1148, 1149, 1150, 1152, 1156, 1158, 1160, 1163, 1164,\n",
       "        1165, 1166, 1167, 1169, 1170, 1171, 1173, 1174, 1176, 1177, 1178,\n",
       "        1179, 1182, 1183, 1185, 1186, 1187, 1188, 1189, 1190, 1191, 1194,\n",
       "        1195, 1198, 1199, 1200, 1201, 1203, 1206, 1207, 1211, 1212, 1213,\n",
       "        1214, 1215, 1216, 1217, 1218, 1220, 1221, 1222, 1223, 1226, 1231,\n",
       "        1236, 1237, 1238, 1239, 1241, 1243, 1244, 1245, 1246, 1247, 1249,\n",
       "        1250, 1251, 1252, 1253, 1256, 1257, 1258, 1259, 1260, 1261],\n",
       "       dtype=int64),)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_false =  np.nonzero(correct_results==False) \n",
    "id_false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "id_f = [   0,    1,    2,    7,    8,   13,   19,   20,   25,   26,   27,\n",
    "          32,   33,   38,   39,   40,   45,   46,   51,   52,   53,   58,\n",
    "          59,   64,   65,   66,   71,   72,   90,   91,   92,   95,   97,\n",
    "          98,  103,  104,  105,  109,  111,  112,  117,  118,  119,  120,\n",
    "         124,  125,  130,  131,  132,  133,  137,  138,  143,  144,  145,\n",
    "         146,  150,  151,  152,  156,  157,  158,  163,  164,  169,  170,\n",
    "         171,  172,  175,  176,  177,  178,  188,  196,  197,  198,  199,\n",
    "         200,  201,  202,  204,  205,  206,  215,  216,  218,  226,  227,\n",
    "         229,  230,  231,  232,  233,  234,  236,  237,  238,  239,  240,\n",
    "         244,  245,  247,  251,  252,  253,  256,  257,  258,  260,  263,\n",
    "         272,  274,  275,  341,  342,  343,  344,  345,  346,  347,  349,\n",
    "         350,  351,  352,  353,  370,  371,  372,  373,  374,  375,  376,\n",
    "         378,  380,  390,  398,  399,  400,  401,  402,  403,  404,  407,\n",
    "         409,  410,  419,  427,  428,  429,  430,  431,  435,  447,  457,\n",
    "         458,  459,  460,  461,  462,  463,  465,  466,  467,  468,  469,\n",
    "         478,  486,  487,  488,  489,  490,  491,  492,  494,  496,  497,\n",
    "         505,  506,  514,  515,  516,  517,  518,  519,  520,  523,  524,\n",
    "         526,  543,  544,  547,  549,  562,  563,  587,  633,  665,  666,\n",
    "         667,  668,  669,  670,  671,  672,  673,  674,  675,  676,  677,\n",
    "         678,  679,  680,  681,  682,  683,  684,  685,  686,  687,  688,\n",
    "         689,  690,  691,  692,  693,  694,  695,  696,  697,  698,  699,\n",
    "         700,  701,  702,  703,  704,  705,  706,  707,  708,  709,  710,\n",
    "         725,  792,  827,  861,  962,  963,  964,  969,  974,  975,  976,\n",
    "         977,  981,  982,  986,  987,  988,  989,  993,  994,  998, 1005,\n",
    "        1010, 1011, 1012, 1013, 1016, 1017, 1018, 1019, 1020, 1021, 1022,\n",
    "        1023, 1024, 1025, 1029, 1030, 1031, 1034, 1035, 1036, 1037, 1041,\n",
    "        1042, 1046, 1047, 1049, 1053, 1054, 1056, 1059, 1123, 1136, 1141,\n",
    "        1143, 1146, 1148, 1149, 1150, 1152, 1156, 1158, 1160, 1163, 1164,\n",
    "        1165, 1166, 1167, 1169, 1170, 1171, 1173, 1174, 1176, 1177, 1178,\n",
    "        1179, 1182, 1183, 1185, 1186, 1187, 1188, 1189, 1190, 1191, 1194,\n",
    "        1195, 1198, 1199, 1200, 1201, 1203, 1206, 1207, 1211, 1212, 1213,\n",
    "        1214, 1215, 1216, 1217, 1218, 1220, 1221, 1222, 1223, 1226, 1231,\n",
    "        1236, 1237, 1238, 1239, 1241, 1243, 1244, 1245, 1246, 1247, 1249,\n",
    "        1250, 1251, 1252, 1253, 1256, 1257, 1258, 1259, 1260, 1261]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62.130177514792905\n",
      "67.82178217821782\n",
      "86.24338624338624\n",
      "56.07476635514019\n",
      "98.27586206896551\n",
      "45.890410958904106\n"
     ]
    }
   ],
   "source": [
    "id1 = [i for i in id_f if i < 169]\n",
    "id2 = [i for i in id_f if i >= 169 and i < (169+404)]\n",
    "id3 = [i for i in id_f if i >= (169+404) and i < (169+404+378)]\n",
    "id4 = [i for i in id_f if i >= (169+404+378) and i < (169+404+378+107)]\n",
    "id5 = [i for i in id_f if i >= (169+404+378+107) and i < (169+404+378+107+58)]\n",
    "id6 = [i for i in id_f if i >= (169+404+378+107+58) and i < 1262]\n",
    "acc1 = (1-len(id1)/169)*100 ; print ( acc1)\n",
    "acc2 = (1-len(id2)/404)*100 ; print ( acc2)\n",
    "acc3 = (1-len(id3)/378)*100 ; print ( acc3)\n",
    "acc4 = (1-len(id4)/107)*100 ; print ( acc4)\n",
    "acc5 = (1-len(id5)/58)*100 ; print ( acc5)\n",
    "acc6 = (1-len(id6)/146)*100 ; print ( acc6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
