{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from file 'E:/05_research/04_deep_learning_model/Save_parameters/modelA/fully_best/modelA_ratio100_set01'...\n",
      "INFO:tensorflow:Restoring parameters from E:/05_research/04_deep_learning_model/Save_parameters/modelA/fully_best/modelA_ratio100_set01\n",
      "Loading validation data...\n",
      "322\n",
      "322\n",
      "Validating model...\n",
      "LT: 79.38144329896907\n",
      "GT: 68.31683168316832\n",
      "TP: 96.3963963963964\n",
      "Overall: 81.87702265372168\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys \n",
    "import numpy as np \n",
    "import tensorflow as tf \n",
    "import time \n",
    "batch_size =50 \n",
    "\n",
    "def load_model(session, save_path):\n",
    "    \"\"\" Loads a saved TF model from a file.Returns:The inputs placehoder \"\"\"\n",
    "    print(\"Loading model from file '%s'...\" % (save_path))\n",
    "    meta_file = save_path + \".meta\"\n",
    "    if not os.path.exists(meta_file):\n",
    "        print(\"ERROR: Expected .meta file '%s', but could not find it.\" % \\\n",
    "        (meta_file))\n",
    "        sys.exit(1) \n",
    "    saver = tf.train.import_meta_graph(meta_file)\n",
    "    save_path = os.path.join(\"./\", save_path) \n",
    "    saver.restore(session, save_path)  \n",
    "    return extract_validation_handles(session)\n",
    "    \n",
    "def load_validation_data( model,filename):\n",
    "    '''load the testing dataset with the process and the name of testing file''' \n",
    "    global batch_size \n",
    "    \"\"\" Loads the validation data,  Returns: A tuple of the loaded validation data and validation labels. \"\"\"\n",
    "    print(\"Loading validation data...\")\n",
    "    folder = 'sub_features/'+ filename  \n",
    "    if model ==1: # SP process\n",
    "        testX =np.load('E:/identification_data/01_PSSE_datasets/Dataset/saved_data_jupyter/'+folder+'/testX_pred.npy' ) \n",
    "        testY =np.load('E:/identification_data/01_PSSE_datasets/Dataset/saved_data_jupyter/'+folder+'/testY_pred.npy'  ) \n",
    "        Energy =np.load('E:/identification_data/01_PSSE_datasets/Dataset/saved_data_jupyter/'+folder+'/Energy_pred.npy' )\n",
    "    elif model==2: # NS process\n",
    "        testX =np.load('E:/identification_data/01_PSSE_datasets/Dataset/saved_data_jupyter/'+folder+'/testX_no.npy' ) \n",
    "        testY =np.load('E:/identification_data/01_PSSE_datasets/Dataset/saved_data_jupyter/'+folder+'/testY_no.npy' ) \n",
    "        Energy =np.load('E:/identification_data/01_PSSE_datasets/Dataset/saved_data_jupyter/'+folder+'/Energy_no.npy'  ) \n",
    "    batch_size =np.shape(testX)[0]  \n",
    "    print (batch_size)\n",
    "    return (testX,testY,Energy)  \n",
    "\n",
    "def validate_model(session, val_data, x,y, e, phase_train,prob ): \n",
    "    \"\"\" Validates the model stored in a session.Returns:The overall validation accuracy for the model. \"\"\"\n",
    "    global batch_size\n",
    "    print(\"Validating model...\") \n",
    "    predict_op = tf.argmax(prob, 1)\n",
    "    correct = tf.equal(predict_op,tf.argmax(y, 1))\n",
    "    acc= tf.reduce_mean(tf.cast(correct, tf.float32))  \n",
    "    val_x, val_y, val_e = val_data\n",
    "    test_correct = session.run(correct,feed_dict={x: val_x, e:val_e,y :val_y , phase_train : True  })\n",
    "    test_acc = session.run(acc,feed_dict={x: val_x, e:val_e,y :val_y , phase_train : True  })\n",
    "    return ( test_acc), test_correct  \n",
    "\n",
    "def extract_validation_handles(session):\n",
    "    \"\"\" Extracts the input and predict_op handles that we use for validation.\n",
    "    Args:\n",
    "      session: The session with the loaded graph.\n",
    "    Returns:\n",
    "      The inputs placeholder  \"\"\" \n",
    "    valid_nodes = tf.get_collection_ref(\"validation_nodes\") \n",
    "    x = valid_nodes[0] \n",
    "    y = valid_nodes[1]\n",
    "    e = valid_nodes[2]  \n",
    "    phase_train =valid_nodes[3]\n",
    "    prob = valid_nodes[4]\n",
    "    global batch_size  \n",
    "    return (x,y,e, phase_train,prob)\n",
    "\n",
    "def each_perform(correct_results,eval_labels ):\n",
    "    label_y=np.argmax(eval_labels,1) \n",
    "    \n",
    "    acc = []\n",
    "    zero = np.where(label_y==0) \n",
    "    correctl=[correct_results[i] for i in zero]\n",
    "    accuracy_zero= np.mean(correctl,dtype=np.float32)\n",
    "    acc.append(accuracy_zero)\n",
    "\n",
    "    one = np.where(label_y==1)\n",
    "    correctg=[correct_results[i] for i in one] \n",
    "    accuracy_one= np.mean(correctg,dtype=np.float32)\n",
    "    acc.append(accuracy_one)\n",
    "\n",
    "    two = np.where(label_y==2)\n",
    "    correctt=[correct_results[i] for i in two] \n",
    "    accuracy_two= np.mean(correctt,dtype=np.float32)\n",
    "    acc.append(accuracy_two) \n",
    "    return acc, correctl,correctg,correctt\n",
    "\n",
    "\n",
    "def reset():\n",
    "    tf.reset_default_graph() \n",
    "model = 2# model = 1: SP process; model = 2: NS process\n",
    "folder ='fully_best/modelA_ratio100_set01'#'ratio_80_best/modelA_ratio80_set4'####'modelA_fully3'##''modelA_ratio100_set3'#'modelA_ratio80_set4'#'modelA_ratio50_set2'#'modelA_ratio30_set2'#'modelA_fully3'#'modelA_ratio24_set05'#'modelA_small04' # 'modelA_fully3' #,'save2','save3','save4','save5','save6','save7','save8','save9','save10','save11','save12','save13','save14','save15' ]\n",
    "testname = 'one_sec'\n",
    "if testname == 'one_sec':\n",
    "    used = [768, 4, 261, 262, 11, 12, 791, 26, 177, 951, 957, 192, 964, 716, 717, 980, 985, 219, 733, 227, 229, 741, 1127, 250, 1135, 752, 753, 1139, 1140, 756, 757, 246, 1146, 1149]\n",
    "elif testname == 'two_sec':\n",
    "    used = [385, 646, 392, 650, 656, 400, 401, 660, 407, 410, 155, 420, 423, 427, 450, 581, 1225, 591, 1237, 1238, 342, 991, 1248, 1249, 352, 1004, 627, 632, 381]\n",
    "elif testname == 'half_sec':\n",
    "    used = [282, 288, 1058, 1063, 1070, 1198, 690, 1203, 1076, 308, 1079, 1208, 315, 700, 64, 708, 709, 1094, 1222, 1098, 76, 79, 1107, 86, 1114, 866, 879, 113, 888, 890]\n",
    "elif testname == 'one_half_sec':\n",
    "    used = [1024, 1159, 1162, 1044, 534, 1046, 536, 1175, 540, 1054, 159, 928, 1185, 548, 549, 933, 40, 941, 46, 561, 52, 821, 949, 950, 572, 834, 463, 854, 479, 481, 501, 510, 1023]\n",
    "with tf.Session() as session:  \n",
    "    best_model = 'E:/05_research/04_deep_learning_model/Save_parameters/modelA/'+folder# os.path.join(save_path,model_name)\n",
    "    x,y,e,phase_train,prob = load_model(session, best_model)\n",
    "    val_data = load_validation_data(model, testname) # the predicted mode\n",
    "    print (np.shape(val_data[0])[0])\n",
    "    total = [i for i in range(np.shape(val_data[0])[0])]\n",
    "    remain = list(set(total) - set(used)); \n",
    "    accuracy ,correct_results= validate_model(session, val_data, x,y,e,phase_train,prob) \n",
    "    remain_results = np.array([correct_results[i] for i in remain] )\n",
    "    labels = [val_data[1][i] for i in remain]\n",
    "    #print (\"Overall validation accuracy is %f \\n\" %(accuracy))\n",
    "    acc,correct_l,correct_g,correct_t= each_perform(remain_results,labels ) \n",
    "    #reset() \n",
    "    session.close()     \n",
    "id_True =  np.nonzero(remain_results==True) \n",
    "overall_acc = (len(id_True[0])/len(remain_results))\n",
    "\n",
    "print ('LT:', 100*np.mean( correct_l))\n",
    "print ('GT:', 100*np.mean( correct_g))\n",
    "print ('TP:', 100*np.mean( correct_t))\n",
    "print ('Overall:', 100*overall_acc) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "id_f  =[ 15,   20,   22,   27,   29,   34,   61,   62,   66,   68,   69,\n",
    "          72,   73,   75,   77,   79,   90,   93,  106,  131,  143,  148,\n",
    "         184,  225,  257,  262,  345,  387,  447,  499,  547,  565,  566,\n",
    "         569,  578,  582,  584,  585,  586,  588,  590,  626,  648,  667,\n",
    "         669,  674,  675,  676,  714,  739,  740,  741,  742,  747,  749,\n",
    "         751,  752,  754,  755,  756,  760,  762,  763,  764,  765,  766,\n",
    "         767,  769,  772,  773,  776,  777,  778,  780,  783,  792,  796,\n",
    "         803,  805,  824,  825,  833,  839,  843,  844,  848,  849,  850,\n",
    "         851,  866,  886,  896,  906,  907,  929,  932,  948,  954,  962,\n",
    "         987, 1008, 1026, 1039, 1040, 1042, 1060, 1063, 1064, 1065, 1066,\n",
    "        1068, 1071, 1078, 1079, 1080, 1098, 1103, 1108, 1110, 1124, 1125,\n",
    "        1126, 1127, 1130, 1131, 1132, 1133, 1134]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
